{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GUI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-c2xel2Euc5"
      },
      "source": [
        "#Testing on new data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zjUTpIlRVeK",
        "outputId": "e60adcb2-d751-4ab8-cfb5-74717cae1574"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uzk8TTOqtlr",
        "outputId": "0f6bffd2-30df-4c30-98fb-8ee252fd9b49"
      },
      "source": [
        "pip install langdetect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYiOAggiqw40",
        "outputId": "53917d2e-9f20-40ae-ee5c-39b556aa5c7f"
      },
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "from googletrans import Translator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.1)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3K0YOmeXtbD"
      },
      "source": [
        "# Function to merge Short and Description column into a new column\n",
        "\n",
        "def createDF(Description, ShortDescription, Caller):\n",
        "  inp = {'Description': [Description], 'Short description': [Shortdescription], 'Caller':[Caller]}\n",
        "  data = pd.DataFrame(data=inp)\n",
        "  return data\n",
        "\n",
        "def descMerge(data):\n",
        "  data['newDescription']=data['Description']+ ' '+data['Short description']\n",
        "  return data['newDescription']\n",
        "\n",
        "# Replace with nan values\n",
        "def descReplaceNan(data):\n",
        "  if (len(str(data['newDescription']))<3):\n",
        "    data['newDescription']=np.nan\n",
        "\n",
        "# Language detection\n",
        "def lan_detect(data):                                        \n",
        "   try:                                                          \n",
        "      return detect(data)                                      \n",
        "   except:                                                       \n",
        "      return 'no'\n",
        "# Function to translate the text to english.\n",
        "def fn_translate(text):\n",
        "  translator = Translator()\n",
        "  translation = translator.translate(text, dest='en')\n",
        "  return translation.text\n",
        " \n",
        "#Remove the rows in the data that have NaN values for newDescription\n",
        "def descDropRow(data):\n",
        "  data = data[data['newDescription'].notna()]\n",
        "  return data\n",
        "#Strip html tags\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "def lower_text(text):\n",
        "  print(text)\n",
        "  lowertext = text.lower()\n",
        "  return lowertext\n",
        "\n",
        "def contract_text(text):\n",
        "    text=text.replace(\"ain't\",\"am not / are not / is not / has not / have not\")\n",
        "    text=text.replace(\"aren't\",\"are not / am not\")\n",
        "    text=text.replace(\"can't\",\"cannot\")\n",
        "    text=text.replace(\"can't've\",\"cannot have\")\n",
        "    text=text.replace(\"'cause\",\"because\")\n",
        "    text=text.replace(\"could've\",\"could have\")\n",
        "    text=text.replace(\"couldn't\",\"could not\")\n",
        "    text=text.replace(\"couldn't've\",\"could not have\")\n",
        "    text=text.replace(\"didn't\",\"did not\")\n",
        "    text=text.replace(\"doesn't\",\"does not\")\n",
        "    text=text.replace(\"don't\",\"do not\")\n",
        "    text=text.replace(\"hadn't\",\"had not\")\n",
        "    text=text.replace(\"hadn't've\",\"had not have\")\n",
        "    text=text.replace(\"hasn't\",\"has not\")\n",
        "    text=text.replace(\"haven't\",\"have not\")\n",
        "    text=text.replace(\"he'd\",\"he had / he would\")\n",
        "    text=text.replace(\"he'd've\",\"he would have\")\n",
        "    text=text.replace(\"he'll\",\"he shall / he will\")\n",
        "    text=text.replace(\"he'll've\",\"he shall have / he will have\")\n",
        "    text=text.replace(\"he's\",\"he has / he is\")\n",
        "    text=text.replace(\"how'd\",\"how did\")\n",
        "    text=text.replace(\"how'd'y\",\"how do you\")\n",
        "    text=text.replace(\"how'll\",\"how will\")\n",
        "    text=text.replace(\"how's\",\"how has / how is / how does\")\n",
        "    text=text.replace(\"I'd\",\"I had / I would\")\n",
        "    text=text.replace(\"I'd've\",\"I would have\")\n",
        "    text=text.replace(\"I'll\",\"I shall / I will\")\n",
        "    text=text.replace(\"I'll've\",\"I shall have / I will have\")\n",
        "    text=text.replace(\"I'm\",\"I am\")\n",
        "    text=text.replace(\"I've\",\"I have\")\n",
        "    text=text.replace(\"isn't\",\"is not\")\n",
        "    text=text.replace(\"it'd\",\"it had / it would\")\n",
        "    text=text.replace(\"it'd've\",\"it would have\")\n",
        "    text=text.replace(\"it'll\",\"it shall / it will\")\n",
        "    text=text.replace(\"it'll've\",\"it shall have / it will have\")\n",
        "    text=text.replace(\"it's\",\"it has / it is\")\n",
        "    text=text.replace(\"let's\",\"let us\")\n",
        "    text=text.replace(\"ma'am\",\"madam\")\n",
        "    text=text.replace(\"mayn't\",\"may not\")\n",
        "    text=text.replace(\"might've\",\"might have\")\n",
        "    text=text.replace(\"mightn't\",\"might not\")\n",
        "    text=text.replace(\"mightn't've\",\"might not have\")\n",
        "    text=text.replace(\"must've\",\"must have\")\n",
        "    text=text.replace(\"mustn't\",\"must not\")\n",
        "    text=text.replace(\"mustn't've\",\"must not have\")\n",
        "    text=text.replace(\"needn't\",\"need not\")\n",
        "    text=text.replace(\"needn't've\",\"need not have\")\n",
        "    text=text.replace(\"o'clock\",\"of the clock\")\n",
        "    text=text.replace(\"oughtn't\",\"ought not\")\n",
        "    text=text.replace(\"oughtn't've\",\"ought not have\")\n",
        "    text=text.replace(\"shan't\",\"shall not\")\n",
        "    text=text.replace(\"sha'n't\",\"shall not\")\n",
        "    text=text.replace(\"shan't've\",\"shall not have\")\n",
        "    text=text.replace(\"she'd\",\"she had / she would\")\n",
        "    text=text.replace(\"she'd've\",\"she would have\")\n",
        "    text=text.replace(\"she'll\",\"she shall / she will\")\n",
        "    text=text.replace(\"she'll've\",\"she shall have / she will have\")\n",
        "    text=text.replace(\"she's\",\"she has / she is\")\n",
        "    text=text.replace(\"should've\",\"should have\")\n",
        "    text=text.replace(\"shouldn't\",\"should not\")\n",
        "    text=text.replace(\"shouldn't've\",\"should not have\")\n",
        "    text=text.replace(\"so've\",\"so have\")\n",
        "    text=text.replace(\"so's\",\"so as / so is\")\n",
        "    text=text.replace(\"that'd\",\"that would / that had\")\n",
        "    text=text.replace(\"that'd've\",\"that would have\")\n",
        "    text=text.replace(\"that's\",\"that has / that is\")\n",
        "    text=text.replace(\"there'd\",\"there had / there would\")\n",
        "    text=text.replace(\"there'd've\",\"there would have\")\n",
        "    text=text.replace(\"there's\",\"there has / there is\")\n",
        "    text=text.replace(\"they'd\",\"they had / they would\")\n",
        "    text=text.replace(\"they'd've\",\"they would have\")\n",
        "    text=text.replace(\"they'll\",\"they shall / they will\")\n",
        "    text=text.replace(\"they'll've\",\"they shall have / they will have\")\n",
        "    text=text.replace(\"they're\",\"they are\")\n",
        "    text=text.replace(\"they've\",\"they have\")\n",
        "    text=text.replace(\"to've\",\"to have\")\n",
        "    text=text.replace(\"wasn't\",\"was not\")\n",
        "    text=text.replace(\"we'd\",\"we had / we would\")\n",
        "    text=text.replace(\"we'd've\",\"we would have\")\n",
        "    text=text.replace(\"we'll\",\"we will\")\n",
        "    text=text.replace(\"we'll've\",\"we will have\")\n",
        "    text=text.replace(\"we're\",\"we are\")\n",
        "    text=text.replace(\"we've\",\"we have\")\n",
        "    text=text.replace(\"weren't\",\"were not\")\n",
        "    text=text.replace(\"what'll\",\"what shall / what will\")\n",
        "    text=text.replace(\"what'll've\",\"what shall have / what will have\")\n",
        "    text=text.replace(\"what're\",\"what are\")\n",
        "    text=text.replace(\"what's\",\"what has / what is\")\n",
        "    text=text.replace(\"what've\",\"what have\")\n",
        "    text=text.replace(\"when's\",\"when has / when is\")\n",
        "    text=text.replace(\"when've\",\"when have\")\n",
        "    text=text.replace(\"where'd\",\"where did\")\n",
        "    text=text.replace(\"where's\",\"where has / where is\")\n",
        "    text=text.replace(\"where've\",\"where have\")\n",
        "    text=text.replace(\"who'll\",\"who shall / who will\")\n",
        "    text=text.replace(\"who'll've\",\"who shall have / who will have\")\n",
        "    text=text.replace(\"who's\",\"who has / who is\")\n",
        "    text=text.replace(\"who've\",\"who have\")\n",
        "    text=text.replace(\"why's\",\"why has / why is\")\n",
        "    text=text.replace(\"why've\",\"why have\")\n",
        "    text=text.replace(\"will've\",\"will have\")\n",
        "    text=text.replace(\"won't\",\"will not\")\n",
        "    text=text.replace(\"won't've\",\"will not have\")\n",
        "    text=text.replace(\"would've\",\"would have\")\n",
        "    text=text.replace(\"wouldn't\",\"would not\")\n",
        "    text=text.replace(\"wouldn't've\",\"would not have\")\n",
        "    text=text.replace(\"y'all\",\"you all\")\n",
        "    text=text.replace(\"y'all'd\",\"you all would\")\n",
        "    text=text.replace(\"y'all'd've\",\"you all would have\")\n",
        "    text=text.replace(\"y'all're\",\"you all are\")\n",
        "    text=text.replace(\"y'all've\",\"you all have\")\n",
        "    text=text.replace(\"you'd\",\"you had / you would\")\n",
        "    text=text.replace(\"you'd've\",\"you would have\")\n",
        "    text=text.replace(\"you'll\",\"you shall / you will\")\n",
        "    text=text.replace(\"you'll've\",\"you shall have / you will have\")\n",
        "    text=text.replace(\"you're\",\"you are\")\n",
        "    text=text.replace(\"you've\",\"you have\")\n",
        "   \n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "#Data cleaning\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text=text.replace(('first name: ').lower(),'firstname')\n",
        "    text=text.replace(('last name: ').lower(),'lastname')\n",
        "    text=text.replace(('received from:').lower(),'') \n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text) #remove email    \n",
        "    text=text.replace('email:','')\n",
        "    text=text.replace('email address:','') \n",
        "    index1=text.find('from:')\n",
        "    index2=text.find('\\nsddubject:')\n",
        "    text=text.replace(text[index1:index2],'')\n",
        "    index3=text.find('[cid:image')\n",
        "    index4=text.find(']')\n",
        "    text=text.replace(text[index3:index4],'')\n",
        "    text=text.replace('this message was sent from an unmonitored email address', '')\n",
        "    text=text.replace('please do not reply to this message', '')\n",
        "    text=text.replace('select the following link to view the disclaimer in an alternate language','')\n",
        "    text=text.replace('description problem', '') \n",
        "    text=text.replace('steps taken far', '')\n",
        "    text=text.replace('customer job title', '')\n",
        "    text=text.replace('sales engineer contact', '')\n",
        "    text=text.replace('description of problem:', '')\n",
        "    text=text.replace('steps taken so far', '')\n",
        "    text=text.replace('please do the needful', '')\n",
        "    text=text.replace('please note that ', '')\n",
        "    text=text.replace('please find below', '')\n",
        "    text=text.replace('date and time', '')\n",
        "    text=text.replace('kindly refer mail', '')\n",
        "    text=text.replace('sincerely', '')\n",
        "    text=text.replace('company inc', '')\n",
        "    text=text.replace('hello', '')\n",
        "    text=text.replace('hallo', '')\n",
        "    text=text.replace('hi it team', '')\n",
        "    text=text.replace('hi team', '')\n",
        "    text=text.replace('hi ', '')\n",
        "    text=text.replace('sir', '')\n",
        "    text=text.replace('best', '')\n",
        "    text=text.replace('kind', '')\n",
        "    text=text.replace('kindly', '')\n",
        "    text=text.replace('yes', '')\n",
        "    text=text.replace('no ', '')\n",
        "    text=text.replace(' able ', '')\n",
        "    text=text.replace('cannot', '')\n",
        "    text=text.replace('regards', '')\n",
        "    text=text.replace('dear', '')\n",
        "    text=text.replace('good morning', '')\n",
        "    text=text.replace('please', '')\n",
        "    text=text.replace('pls', '')\n",
        "    text=text.replace('regards', '')\n",
        "    text=text.replace('please contact', '')\n",
        "    text=text.replace('i am', '')\n",
        "    text=text.replace('help', '')\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'\\w+(?=\\s+:)', ' ', text) # remove word before : sign\n",
        "    text = re.sub(r'\\w+(?=\\:)', ' ', text) # remove word before : sign\n",
        "    text = re.sub(r'^a-zA-z0-9\\s', ' ', text)\n",
        "    custom_punctuation='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~'\n",
        "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
        "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'<.*?>+', ' ', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub(r'\\r\\n', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+.com', ' ',text)\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b', '',text)  #drop words with less than 3 characters\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# remove extra white space\n",
        "def whitesp(text):\n",
        "  pattern = r'[' +',' ']'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text\n",
        "\n",
        "# Split Caller name column\n",
        "def callerSplit(data):\n",
        "  if not data['Caller'].isna().any():\n",
        "    data = data.join(data['Caller'].str.split(' ', 1, expand=True).rename(columns={0:'CallerFirst', 1:'CallerLast'}))\n",
        "  else:\n",
        "    data['CallerFirst'] = \"and\"\n",
        "    data['CallerLast'] = \"and\"\n",
        "  return data  \n",
        "# Caller Name replace with blank space in description section\n",
        "def callerNameReplace(data):\n",
        "  if 'CallerLast' in data:\n",
        "    print()\n",
        "  else:\n",
        "    data['CallerLast']= data['CallerFirst']\n",
        "  text1 = data['newDescription'][0]\n",
        "  text2 = data['CallerFirst'][0]\n",
        "  text3 = data['CallerLast'][0]\n",
        "  replaceDesc1 = text1.replace(text2,\" \")\n",
        "  replaceDesc2 = replaceDesc1.replace(text3,\" \")\n",
        "  data['newDescription'] = replaceDesc2  \n",
        "  return data\n",
        " \n",
        "#Remove stop words\n",
        "def stopWord(data):\n",
        "  data['newDescription'] = data['newDescription'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
        "  return data\n",
        "\n",
        "#Lemmatization\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()]) \n",
        "\n",
        "#Remove white space\n",
        "def whitesp(text):\n",
        "  pattern = r'[' +',' ']'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text\n",
        "  \n",
        "#Lemmatization\n",
        "def lemmatize2(data):\n",
        "  data['newDescription']= data['newDescription'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in str(x).split()]))\n",
        "  return data['newDescription']\n",
        "\n",
        "# words repeatitions found in a description- remove the repetitions\n",
        "def removeRepeats(myText):\n",
        "    l = myText.split()\n",
        "    temp = []\n",
        "    for x in l:\n",
        "        if x not in temp:\n",
        "            temp.append(x)\n",
        "    return ' '.join(temp)\n",
        "\n",
        "\n",
        "# mapping the prediction with the group name\n",
        "def pred_mapping(num,le_name_mapping):\n",
        "  predictedGroup = list(le_name_mapping.keys())[list(le_name_mapping.values()).index(num)]\n",
        "  print(predictedGroup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXCHIn1VXtfy"
      },
      "source": [
        "#Input \n",
        "Description = \"received from: monitoring_tool@company.com job Job_1854 failed in job_scheduler at: 10/31/2016 01:36:00\"\n",
        "Shortdescription = \"\"\n",
        "Caller = \"gordan ramsay\"\n",
        "data = {'Description': [Description], 'Shortdescription': [Shortdescription], 'Caller':[Caller]}\n",
        "df = pd.DataFrame(data=data)\n",
        "df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i74raOsSXtjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "36ac5931-1094-49b5-d96a-cc729f424667"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Shortdescription</th>\n",
              "      <th>Caller</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>received from: monitoring_tool@company.com job...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gordan ramsay</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Description  ...         Caller\n",
              "0  received from: monitoring_tool@company.com job...  ...  gordan ramsay\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsBxxdGsXtm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9ace6b-e73a-4d93-b9d2-8e47812fe3ed"
      },
      "source": [
        "df = createDF(Description, Shortdescription, Caller)\n",
        "\n",
        "df['newDescription'] = descMerge(df)\n",
        "\n",
        "descReplaceNan(df)\n",
        "\n",
        "df['language'] = df['newDescription'].apply(lan_detect)\n",
        "\n",
        "translated = fn_translate(df['newDescription'][0])\n",
        "df['newDescription'] = translated\n",
        "\n",
        "strip_text = strip_html_tags(df['newDescription'][0])\n",
        "df['newDescription'] = strip_text\n",
        "\n",
        "lower_text = lower_text(df['newDescription'][0])\n",
        "df['newDescription'] = lower_text \n",
        "\n",
        "contract_text = contract_text(df['newDescription'][0])\n",
        "df['newDescription'] = contract_text\n",
        "\n",
        "df['newDescription'] = clean_text(df['newDescription'][0])\n",
        "\n",
        "df['newDescription'] = whitesp(df['newDescription'][0])\n",
        "\n",
        "df = callerSplit(df)\n",
        "\n",
        "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "df = callerNameReplace(df)\n",
        "\n",
        "df = stopWord(df)\n",
        "\n",
        "df['newDescription'] = whitesp(df['newDescription'][0])\n",
        "\n",
        "#df['newDescription'] = lemmatize2(df)\n",
        "\n",
        "df['newDescription'] = removeRepeats(df['newDescription'][0])\n",
        "\n",
        "# X value\n",
        "X_new = df['newDescription']\n",
        "print(X_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "received from: monitoring_tool@company.com job Job_1854 failed in job_scheduler at: 10/31/2016 01:36:00\n",
            "\n",
            "0    job failed scheduler\n",
            "Name: newDescription, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zueANlOUbsMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1c4788-e618-4626-92f3-e46957c5e163"
      },
      "source": [
        "print(df['Description'][0])\n",
        "\n",
        "## load the label encoders from disk  -\n",
        "project_path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "filename = 'le_name_mapping.pkl'\n",
        "le_name_mapping1 = pickle.load(open(project_path+filename, 'rb'))\n",
        "filename = 'le_name_mapping_bi.pkl'\n",
        "le_name_mapping_bi = pickle.load(open(project_path+filename, 'rb'))\n",
        "filename = 'le_name_mapping_mu.pkl'\n",
        "le_name_mapping_mu = pickle.load(open(project_path+filename, 'rb'))\n",
        "\n",
        "#Load models and predict\n",
        "\n",
        "#Model#13\n",
        "filename = 'svm_binary_ngram11.sav'\n",
        "loaded_model_svm_binary_ngram11 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm_binary_ngram11 = loaded_model_svm_binary_ngram11.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm_binary_ngram11,le_name_mapping_bi)\n",
        "\n",
        "#Model#14\n",
        "filename = 'svm_binary_ngram12.sav'\n",
        "loaded_model_svm_binary_ngram12 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm_binary_ngram12 = loaded_model_svm_binary_ngram12.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm_binary_ngram12,le_name_mapping_bi)\n",
        "\n",
        "#Model#15\n",
        "filename = 'svm_binary_ngram12_smote.sav'\n",
        "filename2 = 'svm_binary_ngram12_smote_tvect.sav'\n",
        "loaded_model_svm_binary_ngram12_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "loaded_tvect_svm_binary_ngram12_smote = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_svm_binary_ngram12_smote.transform(X_new)\n",
        "y_pred_new_svm_binary_ngram12_smote = loaded_model_svm_binary_ngram12_smote.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_svm_binary_ngram12_smote,le_name_mapping_bi)\n",
        "\n",
        "\n",
        "#Model#1\n",
        "project_path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "filename = 'svm.sav'\n",
        "loaded_model_svm = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm = loaded_model_svm.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm,le_name_mapping1)\n",
        "\n",
        "#Model#2\n",
        "filename = 'svm_ng2.sav'\n",
        "loaded_model_svm_ng2 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm_ng2 = loaded_model_svm_ng2.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm_ng2,le_name_mapping1)\n",
        "\n",
        "#Model#3\n",
        "\n",
        "filename = 'svc_smote.sav'\n",
        "loaded_model_svc_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "filename2 = 'svc_smote_tvect.sav'\n",
        "loaded_tvect_svm = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_svm.transform(X_new)\n",
        "y_pred_new_svc_smote = loaded_model_svc_smote.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_svc_smote,le_name_mapping1)\n",
        "\n",
        "\n",
        "#Model#4\n",
        "filename = 'rfc.sav'\n",
        "loaded_model_rfc = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_rfc = loaded_model_rfc.predict(X_new)\n",
        "pred_mapping(y_pred_new_rfc,le_name_mapping1)\n",
        "\n",
        "#Model#5\n",
        "filename = 'rfc_ng2.sav'\n",
        "loaded_model_rfc_ng2 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_rfc_ng2 = loaded_model_rfc_ng2.predict(X_new)\n",
        "pred_mapping(y_pred_new_rfc_ng2,le_name_mapping1)\n",
        "\n",
        "\n",
        "#Model#6\n",
        "filename = 'rfc_smote.sav'\n",
        "loaded_model_rfc_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "filename2 = 'rfc_smote_tvect.sav'\n",
        "loaded_tvect_rfc = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_rfc.transform(X_new)\n",
        "y_pred_new_rfc_smote = loaded_model_rfc_smote.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_rfc_smote,le_name_mapping1)\n",
        "\n",
        "#Model#7\n",
        "filename = 'gbc.sav'\n",
        "loaded_model_gbc = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_gbc = loaded_model_gbc.predict(X_new)\n",
        "pred_mapping(y_pred_new_gbc,le_name_mapping1)\n",
        "\n",
        "#Model#8\n",
        "filename = 'gbc_ng2.sav'\n",
        "loaded_model_gbc_ng2 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_gbc_ng2 = loaded_model_gbc_ng2.predict(X_new)\n",
        "pred_mapping(y_pred_new_gbc_ng2,le_name_mapping1)\n",
        "\n",
        "\n",
        "#Model#9\n",
        "filename = 'gbc_smote.sav'\n",
        "filename2 = 'gbc_smote_tvect.sav'\n",
        "loaded_model_gbc_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "loaded_tvect_gbc = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_gbc.transform(X_new)\n",
        "y_pred_new_gbc_smote = loaded_model_gbc_smote.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_gbc_smote,le_name_mapping1)\n",
        "\n",
        "\n",
        "#Model#10\n",
        "filename = 'xgbc.sav'\n",
        "loaded_model_xgbc = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_xgbc = loaded_model_xgbc.predict(X_new)\n",
        "pred_mapping(y_pred_new_xgbc,le_name_mapping1)\n",
        "\n",
        "#Model#11\n",
        "filename = 'xgbc_ng2.sav'\n",
        "loaded_model_xgbc_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_xgbc_ng2 = loaded_model_xgbc_smote.predict(X_new)\n",
        "pred_mapping(y_pred_new_xgbc_ng2,le_name_mapping1)\n",
        "\n",
        "#Model#12\n",
        "filename = 'xgbc_smote.sav'\n",
        "filename2 = 'xgbc_smote_tvect.sav'\n",
        "loaded_model_xgbc_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "loaded_tvect_xgbc = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_xgbc.transform(X_new)\n",
        "y_pred_new_xgbc_smote = loaded_model_xgbc_smote.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_xgbc_smote,le_name_mapping1)\n",
        "\n",
        "\n",
        "#Model#16\n",
        "filename = 'svc_multi_smote_ngram11.sav'\n",
        "filename2 = 'svc_multi_smote_ngram11_tvect.sav'\n",
        "loaded_model_svm_multi_ngram11 = pickle.load(open(project_path+filename, 'rb'))\n",
        "loaded_tvect_svm_multi_ngram11 = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_svm_multi_ngram11.transform(X_new)\n",
        "y_pred_new_svm_multi_ngram11 = loaded_model_svm_multi_ngram11.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_svm_multi_ngram11,le_name_mapping_mu)\n",
        "\n",
        "#Model#17\n",
        "filename = 'svc_multi_smote_ngram12.sav'\n",
        "filename2 = 'svc_multi_smote_ngram12_tvect.sav'\n",
        "loaded_model_svm_multi_ngram12 = pickle.load(open(project_path+filename, 'rb'))\n",
        "loaded_tvect_svm_multi_ngram12 = pickle.load(open(project_path+filename2, 'rb'))\n",
        "X_new_tfidf = loaded_tvect_svm_multi_ngram12.transform(X_new)\n",
        "y_pred_new_svm_multi_ngram12 = loaded_model_svm_multi_ngram12.predict(X_new_tfidf)\n",
        "pred_mapping(y_pred_new_svm_multi_ngram12,le_name_mapping_mu)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "received from: monitoring_tool@company.com job Job_1854 failed in job_scheduler at: 10/31/2016 01:36:00\n",
            "GRP_OTHERS\n",
            "GRP_OTHERS\n",
            "GRP_0\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_0\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_0\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_0\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_0\n",
            "GRP_2\n",
            "GRP_12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlqhDetVBdNg"
      },
      "source": [
        "Conclusion the Auto ML model shows that the models trained on SMOTE are not predicting well because they were overfitting.<br> We will use the Binary without SMOTE and and the other models\n",
        "such as SVC, RF, GBC and XGboost for predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK3FjN8nbsV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35d98f8-de46-414b-a350-277607ad9f69"
      },
      "source": [
        "print(df['Description'][0])\n",
        "\n",
        "## load the label encoders from disk  -\n",
        "project_path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "filename = 'le_name_mapping.pkl'\n",
        "le_name_mapping1 = pickle.load(open(project_path+filename, 'rb'))\n",
        "filename = 'le_name_mapping_bi.pkl'\n",
        "le_name_mapping_bi = pickle.load(open(project_path+filename, 'rb'))\n",
        "filename = 'le_name_mapping_mu.pkl'\n",
        "le_name_mapping_mu = pickle.load(open(project_path+filename, 'rb'))\n",
        "\n",
        "#Load models and predict\n",
        "\n",
        "#Model#13\n",
        "filename = 'svm_binary_ngram11.sav'\n",
        "loaded_model_svm_binary_ngram11 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm_binary_ngram11 = loaded_model_svm_binary_ngram11.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm_binary_ngram11,le_name_mapping_bi)\n",
        "\n",
        "#Model#14\n",
        "filename = 'svm_binary_ngram12.sav'\n",
        "loaded_model_svm_binary_ngram12 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm_binary_ngram12 = loaded_model_svm_binary_ngram12.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm_binary_ngram12,le_name_mapping_bi)\n",
        "\n",
        "\n",
        "\n",
        "#Model#1\n",
        "project_path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "filename = 'svm.sav'\n",
        "loaded_model_svm = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm = loaded_model_svm.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm,le_name_mapping1)\n",
        "\n",
        "#Model#2\n",
        "filename = 'svm_ng2.sav'\n",
        "loaded_model_svm_ng2 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_svm_ng2 = loaded_model_svm_ng2.predict(X_new)\n",
        "pred_mapping(y_pred_new_svm_ng2,le_name_mapping1)\n",
        "\n",
        "\n",
        "\n",
        "#Model#4\n",
        "filename = 'rfc.sav'\n",
        "loaded_model_rfc = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_rfc = loaded_model_rfc.predict(X_new)\n",
        "pred_mapping(y_pred_new_rfc,le_name_mapping1)\n",
        "\n",
        "#Model#5\n",
        "filename = 'rfc_ng2.sav'\n",
        "loaded_model_rfc_ng2 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_rfc_ng2 = loaded_model_rfc_ng2.predict(X_new)\n",
        "pred_mapping(y_pred_new_rfc_ng2,le_name_mapping1)\n",
        "\n",
        "\n",
        "\n",
        "#Model#7\n",
        "filename = 'gbc.sav'\n",
        "loaded_model_gbc = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_gbc = loaded_model_gbc.predict(X_new)\n",
        "pred_mapping(y_pred_new_gbc,le_name_mapping1)\n",
        "\n",
        "#Model#8\n",
        "filename = 'gbc_ng2.sav'\n",
        "loaded_model_gbc_ng2 = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_gbc_ng2 = loaded_model_gbc_ng2.predict(X_new)\n",
        "pred_mapping(y_pred_new_gbc_ng2,le_name_mapping1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Model#10\n",
        "filename = 'xgbc.sav'\n",
        "loaded_model_xgbc = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_xgbc = loaded_model_xgbc.predict(X_new)\n",
        "pred_mapping(y_pred_new_xgbc,le_name_mapping1)\n",
        "\n",
        "#Model#11\n",
        "filename = 'xgbc_ng2.sav'\n",
        "loaded_model_xgbc_smote = pickle.load(open(project_path+filename, 'rb'))\n",
        "y_pred_new_xgbc_ng2 = loaded_model_xgbc_smote.predict(X_new)\n",
        "pred_mapping(y_pred_new_xgbc_ng2,le_name_mapping1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "received from: monitoring_tool@company.com job Job_1854 failed in job_scheduler at: 10/31/2016 01:36:00\n",
            "GRP_OTHERS\n",
            "GRP_OTHERS\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_8\n",
            "GRP_8\n"
          ]
        }
      ]
    }
  ]
}